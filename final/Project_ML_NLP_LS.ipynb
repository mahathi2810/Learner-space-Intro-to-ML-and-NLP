{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKdneGhTvc+vx7/ey2I7xu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yMZXNVebihac"},"outputs":[],"source":["%pip install --upgrade datasets"]},{"cell_type":"code","source":["import torch\n","import multiprocessing\n","\n","\n","try:\n","    multiprocessing.set_start_method('spawn', force=True)\n","    print(\"Multiprocessing start method set to 'spawn'.\")\n","except ValueError:\n","\n","    print(\"Multiprocessing start method already set.\")"],"metadata":{"id":"CARFWKs8inDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import (\n","    GPT2LMHeadModel,\n","    GPT2Tokenizer,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForLanguageModeling\n",")\n","from datasets import load_dataset\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score\n","import warnings\n","import os\n","import json\n","from typing import Dict, List, Tuple, Optional\n","import time\n","import gc\n","\n","warnings.filterwarnings('ignore')\n","\n","class OptimizedWikiText2NextWordPredictor:\n","    def __init__(self, model_name=\"gpt2\", max_length=128, use_mixed_precision=\"auto\"):\n","\n","        self.model_name = model_name\n","        self.max_length = max_length\n","        self.wikitext_version = \"wikitext-2-raw-v1\"\n","\n","\n","        self.device, self.mixed_precision = self._setup_device_and_precision(use_mixed_precision)\n","\n","        print(f\"Using device: {self.device}\")\n","        print(f\"Mixed precision: {self.mixed_precision}\")\n","\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","\n","        if self.tokenizer.pad_token is None:\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","\n","        self.model.gradient_checkpointing_enable()\n","\n","\n","        self.model.to(self.device)\n","\n","\n","        self._clear_cache()\n","\n","    def _setup_device_and_precision(self, use_mixed_precision):\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda')\n","            device_name = torch.cuda.get_device_name(0)\n","            memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n","            print(f\"Using GPU: {device_name}\")\n","            print(f\"GPU Memory: {memory_gb:.1f} GB\")\n","\n","\n","            if use_mixed_precision == \"auto\":\n","\n","                if torch.cuda.is_bf16_supported():\n","                    mixed_precision = \"bf16\"\n","                else:\n","                    mixed_precision = \"fp16\"\n","            else:\n","                mixed_precision = use_mixed_precision\n","\n","        elif torch.backends.mps.is_available():\n","            device = torch.device('mps')\n","            print(\"Using Apple Silicon GPU (MPS)\")\n","\n","            mixed_precision = \"no\"\n","            if use_mixed_precision not in [\"auto\", \"no\"]:\n","                print(\"Warning: Mixed precision not supported on MPS, using fp32\")\n","\n","        else:\n","            device = torch.device('cpu')\n","            print(\"Using CPU\")\n","            mixed_precision = \"no\"\n","            if use_mixed_precision not in [\"auto\", \"no\"]:\n","                print(\"Warning: Mixed precision not supported on CPU, using fp32\")\n","\n","        return device, mixed_precision\n","\n","    def _clear_cache(self):\n","\n","        if self.device.type == 'cuda':\n","            torch.cuda.empty_cache()\n","        elif self.device.type == 'mps':\n","            torch.mps.empty_cache()\n","        gc.collect()\n","\n","    def optimize_memory(self):\n","\n","        if self.device.type == 'cuda':\n","\n","            torch.cuda.empty_cache()\n","\n","\n","            try:\n","                torch.cuda.set_per_process_memory_fraction(0.9)\n","            except:\n","                pass\n","\n","        elif self.device.type == 'mps':\n","\n","            torch.mps.empty_cache()\n","\n","\n","        if hasattr(self.model.config, 'use_memory_efficient_attention'):\n","            self.model.config.use_memory_efficient_attention = True\n","\n","    def load_wikitext2_data(self, use_streaming=False, max_samples=None):\n","\n","        print(\"Loading WikiText-2 dataset...\")\n","\n","\n","        if use_streaming:\n","            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", streaming=True)\n","        else:\n","            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","\n","\n","        def filter_text(example):\n","            text = example['text'].strip()\n","            return len(text) > 20 and not text.startswith('=') and not text.startswith('==')\n","\n","        dataset = dataset.filter(filter_text)\n","\n","\n","        if max_samples and not use_streaming:\n","            dataset['train'] = dataset['train'].select(range(min(max_samples, len(dataset['train']))))\n","            dataset['validation'] = dataset['validation'].select(range(min(max_samples//10, len(dataset['validation']))))\n","            dataset['test'] = dataset['test'].select(range(min(max_samples//10, len(dataset['test']))))\n","\n","        def tokenize_function(examples):\n","            texts = [text.strip() for text in examples['text'] if len(text.strip()) > 20]\n","\n","            if not texts:\n","                return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n","\n","\n","            tokenized = self.tokenizer(\n","                texts,\n","                truncation=True,\n","                padding='max_length',\n","                max_length=self.max_length,\n","                return_tensors=None,\n","                add_special_tokens=True\n","            )\n","\n","\n","            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n","\n","            return tokenized\n","\n","\n","        print(\"Tokenizing WikiText-2 dataset...\")\n","\n","\n","        num_proc = 1\n","\n","        tokenized_dataset = dataset.map(\n","            tokenize_function,\n","            batched=True,\n","            batch_size=1000,\n","            remove_columns=dataset[\"train\"].column_names,\n","            desc=\"Tokenizing\",\n","            num_proc=num_proc\n","        )\n","\n","\n","        tokenized_dataset = tokenized_dataset.filter(\n","            lambda x: len(x[\"input_ids\"]) > 10\n","        )\n","\n","        self.train_dataset = tokenized_dataset[\"train\"]\n","        self.val_dataset = tokenized_dataset[\"validation\"]\n","        self.test_dataset = tokenized_dataset[\"test\"]\n","\n","        print(f\"Dataset loaded successfully!\")\n","        print(f\"Train samples: {len(self.train_dataset)}\")\n","        print(f\"Validation samples: {len(self.val_dataset)}\")\n","        print(f\"Test samples: {len(self.test_dataset)}\")\n","\n","\n","        if len(self.train_dataset) > 0:\n","            sample = self.train_dataset[0]\n","            decoded_text = self.tokenizer.decode(sample['input_ids'][:50], skip_special_tokens=True)\n","            print(f\"Sample text: {decoded_text}...\")\n","\n","    def fine_tune_on_wikitext2(self,\n","                               output_dir=\"./wikitext2_fine_tuned_gpt2\",\n","                               epochs=2,\n","                               batch_size=8,\n","                               learning_rate=2e-5,\n","                               warmup_steps=100,\n","                               save_steps=500,\n","                               eval_steps=500,\n","                               logging_steps=50):\n","\n","        print(f\"Starting fine-tuning on WikiText-2 with {self.device} optimizations...\")\n","\n","        self.optimize_memory()\n","\n","\n","        if self.device.type == 'mps':\n","            batch_size = min(batch_size, 4)\n","            print(f\"Adjusted batch size to {batch_size} for MPS\")\n","\n","\n","        training_args = TrainingArguments(\n","            output_dir=output_dir,\n","            overwrite_output_dir=True,\n","            num_train_epochs=epochs,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size,\n","            gradient_accumulation_steps=2,\n","\n","\n","            learning_rate=learning_rate,\n","            weight_decay=0.01,\n","            adam_epsilon=1e-8,\n","            max_grad_norm=1.0,\n","\n","\n","            lr_scheduler_type=\"cosine\",\n","            warmup_steps=warmup_steps,\n","\n","\n","            eval_strategy=\"steps\",\n","            eval_steps=eval_steps,\n","            save_steps=save_steps,\n","            save_total_limit=2,\n","            load_best_model_at_end=True,\n","            metric_for_best_model=\"eval_loss\",\n","            greater_is_better=False,\n","\n","\n","            logging_steps=logging_steps,\n","            logging_strategy=\"steps\",\n","\n","\n","            dataloader_pin_memory=(self.device.type == 'cuda'),\n","            remove_unused_columns=False,\n","            prediction_loss_only=True,\n","\n","\n","            fp16=(self.mixed_precision == \"fp16\"),\n","            bf16=(self.mixed_precision == \"bf16\"),\n","            dataloader_num_workers=1 if self.device.type == 'mps' else 2,\n","\n","\n","            report_to=None,\n","\n","\n","            gradient_checkpointing=True,\n","        )\n","\n","\n","        data_collator = DataCollatorForLanguageModeling(\n","            tokenizer=self.tokenizer,\n","            mlm=False,\n","            return_tensors=\"pt\",\n","        )\n","\n","\n","        trainer = Trainer(\n","            model=self.model,\n","            args=training_args,\n","            data_collator=data_collator,\n","            train_dataset=self.train_dataset,\n","            eval_dataset=self.val_dataset,\n","        )\n","\n","\n","        start_time = time.time()\n","\n","        try:\n","\n","            trainer.train()\n","\n","\n","            trainer.save_model()\n","            self.tokenizer.save_pretrained(output_dir)\n","\n","            training_time = time.time() - start_time\n","            print(f\"Fine-tuning completed in {training_time/60:.2f} minutes!\")\n","            print(f\"Model saved to {output_dir}\")\n","\n","\n","            del trainer\n","            self._clear_cache()\n","\n","        except Exception as e:\n","            print(f\"Training failed with error: {e}\")\n","\n","            self._clear_cache()\n","            raise\n","\n","    def calculate_perplexity_fast(self, dataset, batch_size=16, max_samples=1000):\n","\n","        self.model.eval()\n","\n","\n","        if self.device.type == 'mps':\n","            batch_size = min(batch_size, 8)\n","\n","\n","        if max_samples and len(dataset) > max_samples:\n","            dataset = dataset.select(range(max_samples))\n","\n","\n","        data_collator = DataCollatorForLanguageModeling(\n","            tokenizer=self.tokenizer,\n","            mlm=False,\n","        )\n","\n","        dataloader = DataLoader(\n","            dataset,\n","            batch_size=batch_size,\n","            shuffle=False,\n","            collate_fn=data_collator,\n","            pin_memory=(self.device.type == 'cuda')\n","        )\n","\n","        total_loss = 0\n","        num_batches = 0\n","\n","        with torch.no_grad():\n","            for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n","                input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n","                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n","                labels = batch['labels'].to(self.device, non_blocking=True)\n","\n","                outputs = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    labels=labels\n","                )\n","\n","                loss = outputs.loss\n","                total_loss += loss.item()\n","                num_batches += 1\n","\n","        avg_loss = total_loss / num_batches\n","        perplexity = torch.exp(torch.tensor(avg_loss))\n","\n","        return perplexity.item()\n","\n","    def calculate_top_k_accuracy_fast(self, dataset, k=5, batch_size=16, max_samples=1000):\n","\n","        self.model.eval()\n","\n","\n","        if self.device.type == 'mps':\n","            batch_size = min(batch_size, 4)\n","\n","\n","        if max_samples and len(dataset) > max_samples:\n","            dataset = dataset.select(range(max_samples))\n","\n","        correct_predictions = 0\n","        total_predictions = 0\n","\n","        with torch.no_grad():\n","            for i in tqdm(range(0, len(dataset), batch_size), desc=f\"Calculating top-{k} accuracy\"):\n","                batch_end = min(i + batch_size, len(dataset))\n","                batch_data = dataset[i:batch_end]\n","\n","\n","                if isinstance(batch_data['input_ids'], list):\n","                    input_ids = torch.stack([torch.tensor(seq) for seq in batch_data['input_ids']])\n","                else:\n","                    input_ids = batch_data['input_ids']\n","\n","                input_ids = input_ids.to(self.device, non_blocking=True)\n","\n","\n","                for seq_idx in range(input_ids.size(0)):\n","                    sequence = input_ids[seq_idx]\n","\n","\n","                    if len(sequence) < 10:\n","                        continue\n","\n","\n","                    sequence = sequence[:50]\n","\n","\n","                    for pos in range(1, len(sequence) - 1):\n","                        input_seq = sequence[:pos].unsqueeze(0)\n","                        target_token = sequence[pos].item()\n","\n","                        outputs = self.model(input_seq)\n","                        logits = outputs.logits[0, -1, :]\n","\n","\n","                        top_k_tokens = torch.topk(logits, k, dim=-1).indices\n","\n","\n","                        if target_token in top_k_tokens:\n","                            correct_predictions += 1\n","                        total_predictions += 1\n","\n","        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n","        return accuracy\n","\n","    def predict_next_word(self, text, num_predictions=5, temperature=0.7):\n","\n","        self.model.eval()\n","\n","\n","        inputs = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n","\n","        with torch.no_grad():\n","            outputs = self.model(inputs)\n","            predictions = outputs.logits[0, -1, :]\n","\n","\n","            predictions = predictions / temperature\n","\n","\n","            probabilities = torch.softmax(predictions, dim=-1)\n","\n","            top_predictions = torch.topk(probabilities, num_predictions)\n","\n","            results = []\n","            for i in range(num_predictions):\n","                token_id = top_predictions.indices[i].item()\n","                probability = top_predictions.values[i].item()\n","                word = self.tokenizer.decode([token_id])\n","                results.append((word, probability))\n","\n","            return results\n","\n","    def generate_text(self, prompt, max_length=100, temperature=0.7, do_sample=True):\n","\n","        self.model.eval()\n","\n","        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n","\n","        with torch.no_grad():\n","            outputs = self.model.generate(\n","                inputs,\n","                max_length=max_length,\n","                temperature=temperature,\n","                do_sample=do_sample,\n","                pad_token_id=self.tokenizer.eos_token_id,\n","                num_return_sequences=1,\n","                no_repeat_ngram_size=2,\n","                top_k=50,\n","                top_p=0.95\n","            )\n","\n","            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            return generated_text\n","\n","    def evaluate_fast(self, sample_size=500):\n","\n","        print(\"Running fast evaluation on WikiText-2...\")\n","\n","        if self.device.type == 'mps':\n","            sample_size = min(sample_size, 200)\n","\n","        val_subset = self.val_dataset.select(range(min(sample_size, len(self.val_dataset))))\n","        test_subset = self.test_dataset.select(range(min(sample_size, len(self.test_dataset))))\n","\n","        val_perplexity = self.calculate_perplexity_fast(val_subset, batch_size=16, max_samples=sample_size)\n","        test_perplexity = self.calculate_perplexity_fast(test_subset, batch_size=16, max_samples=sample_size)\n","\n","        accuracy_sample_size = min(100, sample_size // 5)\n","        val_top1_acc = self.calculate_top_k_accuracy_fast(val_subset, k=1, max_samples=accuracy_sample_size)\n","        val_top5_acc = self.calculate_top_k_accuracy_fast(val_subset, k=5, max_samples=accuracy_sample_size)\n","\n","        test_top1_acc = self.calculate_top_k_accuracy_fast(test_subset, k=1, max_samples=accuracy_sample_size)\n","        test_top5_acc = self.calculate_top_k_accuracy_fast(test_subset, k=5, max_samples=accuracy_sample_size)\n","\n","        results = {\n","            'perplexity': {\n","                'validation': val_perplexity,\n","                'test': test_perplexity\n","            },\n","            'top1_accuracy': {\n","                'validation': val_top1_acc,\n","                'test': test_top1_acc\n","            },\n","            'top5_accuracy': {\n","                'validation': val_top5_acc,\n","                'test': test_top5_acc\n","            }\n","        }\n","\n","        return results\n","\n","    def plot_results(self, results):\n","\n","        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        datasets = ['validation', 'test']\n","        perplexities = [results['perplexity'][d] for d in datasets]\n","\n","        axes[0].bar(datasets, perplexities, color=['orange', 'green'])\n","        axes[0].set_title('Perplexity on WikiText-2')\n","        axes[0].set_ylabel('Perplexity')\n","        axes[0].set_ylim(0, max(perplexities) * 1.1)\n","\n","        top1_accs = [results['top1_accuracy'][d] for d in datasets]\n","        axes[1].bar(datasets, top1_accs, color=['orange', 'green'])\n","        axes[1].set_title('Top-1 Accuracy on WikiText-2')\n","        axes[1].set_ylabel('Accuracy')\n","        axes[1].set_ylim(0, 1)\n","\n","        top5_accs = [results['top5_accuracy'][d] for d in datasets]\n","        axes[2].bar(datasets, top5_accs, color=['orange', 'green'])\n","        axes[2].set_title('Top-5 Accuracy on WikiText-2')\n","        axes[2].set_ylabel('Accuracy')\n","        axes[2].set_ylim(0, 1)\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def save_model(self, output_dir):\n","\n","        os.makedirs(output_dir, exist_ok=True)\n","        self.model.save_pretrained(output_dir)\n","        self.tokenizer.save_pretrained(output_dir)\n","        print(f\"Model saved to {output_dir}\")\n","\n","    def load_model(self, model_dir):\n","\n","        self.model = GPT2LMHeadModel.from_pretrained(model_dir)\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","        self.model.to(self.device)\n","        print(f\"Model loaded from {model_dir}\")\n","\n","def main():\n","\n","    print(\"=== Optimized WikiText-2 Next Word Predictor ===\")\n","    print(\"Initializing predictor...\")\n","\n","    predictor = OptimizedWikiText2NextWordPredictor(\n","        model_name=\"gpt2\",\n","        max_length=128,\n","        use_mixed_precision=\"auto\"\n","    )\n","\n","    predictor.load_wikitext2_data(\n","        use_streaming=False,\n","        max_samples=5000\n","    )\n","\n","    print(\"\\nStarting fine-tuning with device-specific optimizations...\")\n","    predictor.fine_tune_on_wikitext2(\n","        epochs=1,\n","        batch_size=4,\n","        learning_rate=2e-5,\n","        warmup_steps=50,\n","        save_steps=500,\n","        eval_steps=500,\n","        logging_steps=50\n","    )\n","\n","    print(\"\\n=== Next Word Prediction Demo ===\")\n","    test_sentences = [\n","        \"The history of artificial intelligence\",\n","        \"In the field of computer science\",\n","        \"Machine learning algorithms are\",\n","        \"The research shows that\",\n","        \"According to the study\"\n","    ]\n","\n","    for sentence in test_sentences:\n","        predictions = predictor.predict_next_word(sentence, num_predictions=5)\n","        print(f\"\\nInput: '{sentence}'\")\n","        print(\"Top 5 predictions:\")\n","        for i, (word, prob) in enumerate(predictions, 1):\n","            print(f\"  {i}. '{word}' (probability: {prob:.4f})\")\n","\n","    print(\"\\n=== Text Generation Demo ===\")\n","    prompts = [\n","        \"The concept of machine learning\",\n","        \"In computer science, algorithms\",\n","        \"The development of artificial intelligence\",\n","        \"Research in natural language processing\"\n","    ]\n","\n","    for prompt in prompts:\n","        generated = predictor.generate_text(prompt, max_length=80)\n","        print(f\"\\nPrompt: '{prompt}'\")\n","        print(f\"Generated: '{generated}'\")\n","\n","    print(\"\\n=== Fast Model Evaluation on WikiText-2 ===\")\n","    print(\"Running fast evaluation...\")\n","\n","    results = predictor.evaluate_fast(sample_size=200)\n","\n","    print(\"\\nEvaluation Results:\")\n","    print(f\"Validation Perplexity: {results['perplexity']['validation']:.4f}\")\n","    print(f\"Test Perplexity: {results['perplexity']['test']:.4f}\")\n","    print(f\"Validation Top-1 Accuracy: {results['top1_accuracy']['validation']:.4f}\")\n","    print(f\"Test Top-1 Accuracy: {results['top1_accuracy']['test']:.4f}\")\n","    print(f\"Validation Top-5 Accuracy: {results['top5_accuracy']['validation']:.4f}\")\n","    print(f\"Test Top-5 Accuracy: {results['top5_accuracy']['test']:.4f}\")\n","\n","    try:\n","        predictor.plot_results(results)\n","    except Exception as e:\n","        print(f\"Plotting failed: {e}\")\n","\n","    predictor.save_model(\"./optimized_wikitext2_model\")\n","\n","    print(f\"\\n=== Training Complete! ===\")\n","    print(f\"Training completed on {predictor.device}!\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"4Kw-axYSipl_"},"execution_count":null,"outputs":[]}]}